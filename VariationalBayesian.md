
# EM Algorithm and Variational Bayesian Inference

Here I provide a concise description of the general idea of the methods.

In short, both methods serve to estimate parameters in a statistical model with partially observed samples taken from a distribution.

Suppose you have a bunch of samples $\{x_1, x_2, ..., x_N\}$ generated by some unknown distribution $p(x)$. Firstly, Choose an adequate family of possible distributions. This is your *statistical model*. It can be something simple like family of Gaussian distributions or something complicated like family of neural networks with stochastic input. Typically, these models are parametrized with a real-valued vector of parameters $\mathbf{\theta}$. Our goal is to find an algorithm that estimates $\mathbf{\theta}$ based on given data.

## Maximum Log-Likelihood Estimator

Let's begin with something simple. Here I explain well known method in a not so well known way.

We have a family of distributions $p(x | \mathbf{\theta})$. Our goal is to choose "the best" one. What should we do?
