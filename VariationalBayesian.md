{% include head.html %}

# EM Algorithm and Variational Bayesian Inference

Here I provide a concise description of the general idea of the methods.

In short, both methods serve to estimate parameters in a statistical model with partially observed samples taken from a distribution.

Suppose you have a bunch of samples $\{x_1, x_2, ..., x_N\}$ from an arbitrary probability space
generated by some unknown distribution $p(x)$. Firstly, Choose an adequate family of possible distributions.
This is your *statistical model*.
It can be something simple like family of Gaussian distributions or something complicated like
family of neural networks with stochastic input.
Typically, these models are parametrized with a real-valued vector of parameters $\mathbf{\theta}$.
Our goal is to find an algorithm that estimates $\mathbf{\theta}$ based on given data.

## Maximum Log-Likelihood Estimator

Let's begin with something simple. Here I explain well known method in a not so well known way.

We have a family of distributions $p(x \vert \mathbf{\theta})$. Our goal is to choose "the best" one.
What should we do? Firstly, let's consider a wider Family of probability distributions.
Namely, all possible probability measures over considered probability space.
Then, the best distribution which fits our data is the average of delta measures $\delta(x - x_k)$
for each data point. This distribution is called *empirical distribution* of data.
It doesn't actually have a pdf, but with slight abuse of the notation it could be introduced as

$$
q(x) = \frac{1}{N} \sum_{k=1}^N{\delta(x - x_k)}
$$

Then, simply find the distribution $p(x \vert \theta^*)$ in our statistical model which minimizes KL-divergence
from $q(x)$ (Later I will write a post about different divergences as measures of dissimilarity between
distributions and why we typically choose KL-divergence).

![Schematic depiction of the relation between empirical distribution $q(x)$ and statistical model $p(x \vert \theta)$]({{ site.url }}/kl_no_hidden.pdf)

So far we have

$$
D_{KL}(q(x) \vert \vert p(x \vert \theta)) = \int_{X}{q(x) \log{\frac{q(x)}{p(x \vert \theta)}} dx}
$$
